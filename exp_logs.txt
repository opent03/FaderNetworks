### This file contains notes of ablation experiments;

Wed Apr. 3: We set image size to be 128x128 due to RAM constraints. We trained a new classiifer on attributes Male, Smiling, Bald, Mustache, No_Beard, Young, and Eyeglasses, used the standard hyperparameters for the classifier, and then trained for 500 epochs, 50k images as epoch size (i.e. we don't use all of the training samples, but again its just a classifier to assess. For more details, read the paper)

Then, we trained a fader network with attributes "male" only. We haven't figured out why training it on multiple features would not work because some assertions fail in the interpolation.py class. Apparently we can only interpolate one attribute at the same time. However, in the original github's .../images/ directory, there were a couple of photos showcasing multiple interpolations on different attributes at the same time. I don't know how they got it to work with their current interpolation code, but we'll figure it out. 

As for the fader network, we trained with the following parameters, trying to reproduce the original results:
  
python3 train.py --img_sz 128 --eval_clf models/newclf128/clf/best.pth --attr "Male" --name "fader1_male"

We actually stopped training at 500+ epochs because we noticed that the different losses weren't as frequent as the first hundred epochs or so. 
During the interpolation, some images were sort of messed up, but that's to be expected because we couldn't acquire the 256x256 images dataset, due to limited RAM memory. Therefore, we speculate that the low quality of reconstructed + feature modified images are due to hyperparameters not tuned properly. 

One interesting observation that came out of our broken experiment: 
On the failed images (our feature of choice was female->male, or vice versa), because the images felt a bit funky and definitely generated, we could observe a sort of juxtaposition between the original images of women and the same generic male figure (with mustache, dark hair, dark eyebrows). This is incredibly interesting because it gives insight into what the network is actually doing; we therefore hypothesize that the network, through excessive training, has been able to infer a "generic male" template, and sticks the template at various degrees onto images of women, as represented by this slider effects. 


Sat Apr. 6: We decide to reproduce the experiment for the "Male" attribute only. We have experimented with MSE and MAE losses, and currently running huber loss. 
We ran this command:

TRAIN
python3 train.py --name "male_mae" --img_sz 128 --attr "Male" --n_epochs 100 --epoch_size 120000 --eval_clf '/home/viet/FaderNetworks/models/default/clf_male_smiling_bald_mustache_nobeard_young_eyeglasses/best.pth' --lambda_schedule 300000 --loss_type "mae" 

TEST
python3 interpolate.py --model_path '/home/viet/FaderNetworks/models/male_mse/male_mse_1/best_accu_ae.pth' --n_images 10 --n_interpolations 10 --alpha_min 2.0 --alpha_max 2.0 --output_path stuff/male_mse.png

and change the arguments. 
We do not see an obvious difference between the two losses, aside from the fact that they are both blurrier than the original photo, but that's to be expected because there is always going to be some reconstruction loss along the way. We wonder if adding skip connections would help with the reconstruction quality. The original authors of the paper have already included parameters and code for adding skip connections. So far, our only remark would be that changing losses do not necessarily contribute to better quality or better inference. So far the inferences look the same. 
To-do-list: Train a classifier to recognize male-female on a different dataset. 

